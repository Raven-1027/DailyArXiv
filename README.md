# Daily Papers
The project automatically fetches the latest papers from arXiv based on keywords.

The subheadings in the README file represent the search keywords.

Only the most recent articles for each keyword are retained, up to a maximum of 100 papers.

You can click the 'Watch' button to receive daily email notifications.

Last update: 2025-11-14

## strong correlation
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Capacity Scaling of Massive MIMO in Strong Spatial Correlation Regimes](https://arxiv.org/pdf/1812.08898v2)** | 2019-12-10 | <details><summary>Show</summary><p>This paper investigates the capacity scaling of multicell massive MIMO systems in the presence of spatially correlated fading. In particular, we focus on the strong spatial correlation regimes where the covariance matrix of each user channel vector has a rank that scales sublinearly with the number of base station antennas, as the latter grows to infinity. We also consider the case where the covariance eigenvectors corresponding to the non-zero eigenvalues span randomly selected subspaces. For this channel model, referred to as the "random sparse angular support" model, we characterize the asymptotic capacity scaling law in the limit of large number of antennas. To achieve the asymptotic capacity results, statistical spatial despreading based on the second-order channel statistics plays a pivotal role in terms of pilot decontamination and interference suppression. A remarkable result is that even when the number of users scales linearly with base station antennas, a linear growth of the capacity with respect to the number of antennas is achievable under the sparse angular support model. We note that the achievable rate lower bound based on massive MIMO "channel hardening", widely used in the massive MIMO literature, yields rather loose results in the strong spatial correlation regimes and may significantly underestimate the achievable rate of massive MIMO. This work therefore considers an alternative bounding technique which is better suited to the strong correlation regimes. In fading channels with sparse angular support, it is further shown that spatial despreading (spreading) in uplink (downlink) has a more prominent impact on the performance of massive MIMO than channel hardening.</p></details> | <details><summary>accep...</summary><p>accepted for publication in IEEE Trans. Information Theory</p></details> |
| **[Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization](https://arxiv.org/pdf/2107.04649v2)** | 2021-10-11 | <details><summary>Show</summary><p>For machine learning systems to be reliable, we must understand their performance in unseen, out-of-distribution environments. In this paper, we empirically show that out-of-distribution performance is strongly correlated with in-distribution performance for a wide range of models and distribution shifts. Specifically, we demonstrate strong correlations between in-distribution and out-of-distribution performance on variants of CIFAR-10 & ImageNet, a synthetic pose estimation task derived from YCB objects, satellite imagery classification in FMoW-WILDS, and wildlife classification in iWildCam-WILDS. The strong correlations hold across model architectures, hyperparameters, training set size, and training duration, and are more precise than what is expected from existing domain adaptation theory. To complete the picture, we also investigate cases where the correlation is weaker, for instance some synthetic distribution shifts from CIFAR-10-C and the tissue classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory based on a Gaussian data model that shows how changes in the data covariance arising from distribution shift can affect the observed correlations.</p></details> |  |
| **[Does Weak-to-strong Generalization Happen under Spurious Correlations?](https://arxiv.org/pdf/2509.24005v1)** | 2025-09-30 | <details><summary>Show</summary><p>We initiate a unified theoretical and algorithmic study of a key problem in weak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained student with pseudolabels from a weaker teacher on a downstream task with spurious correlations, does W2S happen, and how to improve it upon failures? We consider two sources of spurious correlations caused by group imbalance: (i) a weak teacher fine-tuned on group-imbalanced labeled data with a minority group of fraction $η_\ell$, and (ii) a group-imbalanced unlabeled set pseudolabeled by the teacher with a minority group of fraction $η_u$. Theoretically, a precise characterization of W2S gain at the proportional asymptotic limit shows that W2S always happens with sufficient pseudolabels when $η_u = η_\ell$ but may fail when $η_u \ne η_\ell$, where W2S gain diminishes as $(η_u - η_\ell)^2$ increases. Our theory is corroborated by extensive experiments on various spurious correlation benchmarks and teacher-student pairs. To boost W2S performance upon failures, we further propose a simple, effective algorithmic remedy that retrains the strong student on its high-confidence data subset after W2S fine-tuning. Our algorithm is group-label-free and achieves consistent, substantial improvements over vanilla W2S fine-tuning.</p></details> |  |

## computational chemistry
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Deep Learning for Computational Chemistry](https://arxiv.org/pdf/1701.04503v1)** | 2018-08-15 | <details><summary>Show</summary><p>The rise and fall of artificial neural networks is well documented in the scientific literature of both computer science and computational chemistry. Yet almost two decades later, we are now seeing a resurgence of interest in deep learning, a machine learning algorithm based on multilayer neural networks. Within the last few years, we have seen the transformative impact of deep learning in many domains, particularly in speech recognition and computer vision, to the extent that the majority of expert practitioners in those field are now regularly eschewing prior established models in favor of deep learning models. In this review, we provide an introductory overview into the theory of deep neural networks and their unique properties that distinguish them from traditional machine learning algorithms used in cheminformatics. By providing an overview of the variety of emerging applications of deep neural networks, we highlight its ubiquity and broad applicability to a wide range of challenges in the field, including QSAR, virtual screening, protein structure prediction, quantum chemistry, materials design and property prediction. In reviewing the performance of deep neural networks, we observed a consistent outperformance against non-neural networks state-of-the-art models across disparate research topics, and deep neural network based models often exceeded the "glass ceiling" expectations of their respective tasks. Coupled with the maturity of GPU-accelerated computing for training deep neural networks and the exponential growth of chemical data on which to train these networks on, we anticipate that deep learning algorithms will be a valuable tool for computational chemistry.</p></details> |  |
| **[Pushing the Limits of Quantum Computing for Simulating PFAS Chemistry](https://arxiv.org/pdf/2311.01242v1)** | 2023-11-03 | <details><summary>Show</summary><p>Accurate and scalable methods for computational quantum chemistry can accelerate research and development in many fields, ranging from drug discovery to advanced material design. Solving the electronic Schrodinger equation is the core problem of computational chemistry. However, the combinatorial complexity of this problem makes it intractable to find exact solutions, except for very small systems. The idea of quantum computing originated from this computational challenge in simulating quantum-mechanics. We propose an end-to-end quantum chemistry pipeline based on the variational quantum eigensolver (VQE) algorithm and integrated with both HPC-based simulators and a trapped-ion quantum computer. Our platform orchestrates hundreds of simulation jobs on compute resources to efficiently complete a set of ab initio chemistry experiments with a wide range of parameterization. Per- and poly-fluoroalkyl substances (PFAS) are a large family of human-made chemicals that pose a major environmental and health issue globally. Our simulations includes breaking a Carbon-Fluorine bond in trifluoroacetic acid (TFA), a common PFAS chemical. This is a common pathway towards destruction and removal of PFAS. Molecules are modeled on both a quantum simulator and a trapped-ion quantum computer, specifically IonQ Aria. Using basic error mitigation techniques, the 11-qubit TFA model (56 entangling gates) on IonQ Aria yields near-quantitative results with milli-Hartree accuracy. Our novel results show the current state and future projections for quantum computing in solving the electronic structure problem, push the boundaries for the VQE algorithm and quantum computers, and facilitates development of quantum chemistry workflows.</p></details> |  |
| **[The Chemistry Between High School Students and Computer Science](https://arxiv.org/pdf/1406.2222v1)** | 2014-06-10 | <details><summary>Show</summary><p>Computer science enrollments have started to rise again, but the percentage of women undergraduates in computer science is still low. Some studies indicate this might be due to a lack of awareness of computer science at the high school level. We present our experiences running a 5-year, high school outreach program that introduces information about computer science within the context of required chemistry courses. We developed interactive worksheets using Molecular Workbench that help the students learn chemistry and computer science concepts related to relevant events such as the gulf oil spill. Our evaluation of the effectiveness of this approach indicates that the students do become more aware of computer science as a discipline, but system support issues in the classroom can make the approach difficult for teachers and discouraging for the students.</p></details> | 8 pages, 2 figures |
| **[Software-Hardware Co-Optimization for Computational Chemistry on Superconducting Quantum Processors](https://arxiv.org/pdf/2105.07127v1)** | 2021-05-18 | <details><summary>Show</summary><p>Computational chemistry is the leading application to demonstrate the advantage of quantum computing in the near term. However, large-scale simulation of chemical systems on quantum computers is currently hindered due to a mismatch between the computational resource needs of the program and those available in today's technology. In this paper we argue that significant new optimizations can be discovered by co-designing the application, compiler, and hardware. We show that multiple optimization objectives can be coordinated through the key abstraction layer of Pauli strings, which are the basic building blocks of computational chemistry programs. In particular, we leverage Pauli strings to identify critical program components that can be used to compress program size with minimal loss of accuracy. We also leverage the structure of Pauli string simulation circuits to tailor a novel hardware architecture and compiler, leading to significant execution overhead reduction by up to 99%. While exploiting the high-level domain knowledge reveals significant optimization opportunities, our hardware/software framework is not tied to a particular program instance and can accommodate the full family of computational chemistry problems with such structure. We believe the co-design lessons of this study can be extended to other domains and hardware technologies to hasten the onset of quantum advantage.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 11 figures, to appear in ISCA 2021</p></details> |
| **[Generative artificial intelligence for computational chemistry: a roadmap to predicting emergent phenomena](https://arxiv.org/pdf/2409.03118v1)** | 2024-09-06 | <details><summary>Show</summary><p>The recent surge in Generative Artificial Intelligence (AI) has introduced exciting possibilities for computational chemistry. Generative AI methods have made significant progress in sampling molecular structures across chemical species, developing force fields, and speeding up simulations. This Perspective offers a structured overview, beginning with the fundamental theoretical concepts in both Generative AI and computational chemistry. It then covers widely used Generative AI methods, including autoencoders, generative adversarial networks, reinforcement learning, flow models and language models, and highlights their selected applications in diverse areas including force field development, and protein/RNA structure prediction. A key focus is on the challenges these methods face before they become truly predictive, particularly in predicting emergent chemical phenomena. We believe that the ultimate goal of a simulation method or theory is to predict phenomena not seen before, and that Generative AI should be subject to these same standards before it is deemed useful for chemistry. We suggest that to overcome these challenges, future AI models need to integrate core chemical principles, especially from statistical mechanics.</p></details> |  |
| **[ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/pdf/2506.06363v1)** | 2025-06-11 | <details><summary>Show</summary><p>Atomistic simulations are essential tools in chemistry and materials science, accelerating the discovery of novel catalysts, energy storage materials, and pharmaceuticals. However, running these simulations remains challenging due to the wide range of computational methods, diverse software ecosystems, and the need for expert knowledge and manual effort for the setup, execution, and validation stages. In this work, we present ChemGraph, an agentic framework powered by artificial intelligence and state-of-the-art simulation tools to streamline and automate computational chemistry and materials science workflows. ChemGraph leverages graph neural network-based foundation models for accurate yet computationally efficient calculations and large language models (LLMs) for natural language understanding, task planning, and scientific reasoning to provide an intuitive and interactive interface. Users can perform tasks such as molecular structure generation, single-point energy, geometry optimization, vibrational analysis, and thermochemistry calculations with methods ranging from tight-binding and machine learning interatomic potentials to density functional theory or wave function theory-based methods. We evaluate ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs (GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows, while more complex tasks benefit from using larger models like GPT-4o. Importantly, we show that decomposing complex tasks into smaller subtasks through a multi-agent framework enables smaller LLM models to match or exceed GPT-4o's performance in specific scenarios.</p></details> |  |
| **[Q# and NWChem: Tools for Scalable Quantum Chemistry on Quantum Computers](https://arxiv.org/pdf/1904.01131v1)** | 2019-05-03 | <details><summary>Show</summary><p>Fault-tolerant quantum computation promises to solve outstanding problems in quantum chemistry within the next decade. Realizing this promise requires scalable tools that allow users to translate descriptions of electronic structure problems to optimized quantum gate sequences executed on physical hardware, without requiring specialized quantum computing knowledge. To this end, we present a quantum chemistry library, under the open-source MIT license, that implements and enables straightforward use of state-of-art quantum simulation algorithms. The library is implemented in Q#, a language designed to express quantum algorithms at scale, and interfaces with NWChem, a leading electronic structure package. We define a standardized schema for this interface, Broombridge, that describes second-quantized Hamiltonians, along with metadata required for effective quantum simulation, such as trial wavefunction ansatzes. This schema is generated for arbitrary molecules by NWChem, conveniently accessible, for instance, through Docker containers and a recently developed web interface EMSL Arrows. We illustrate use of the library with various examples, including ground- and excited-state calculations for LiH, H$_{10}$, and C$_{20}$ with an active-space simplification, and automatically obtain resource estimates for classically intractable examples.</p></details> | <details><summary>36 pa...</summary><p>36 pages, 5 figures. Examples and data in ancillary files folder</p></details> |
| **[An Intermediate Level of Abstraction for Computational Systems Chemistry](https://arxiv.org/pdf/1701.09097v1)** | 2018-02-07 | <details><summary>Show</summary><p>Computational techniques are required for narrowing down the vast space of possibilities to plausible prebiotic scenarios, since precise information on the molecular composition, the dominant reaction chemistry, and the conditions for that era are scarce. The exploration of large chemical reaction networks is a central aspect in this endeavour. While quantum chemical methods can accurately predict the structures and reactivities of small molecules, they are not efficient enough to cope with large-scale reaction systems. The formalization of chemical reactions as graph grammars provides a generative system, well grounded in category theory, at the right level of abstraction for the analysis of large and complex reaction networks. An extension of the basic formalism into the realm of integer hyperflows allows for the identification of complex reaction patterns, such as auto-catalysis, in large reaction networks using optimization techniques.</p></details> |  |
| **[MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows](https://arxiv.org/pdf/2310.20155v1)** | 2024-02-05 | <details><summary>Show</summary><p>Machine learning (ML) is increasingly becoming a common tool in computational chemistry. At the same time, the rapid development of ML methods requires a flexible software framework for designing custom workflows. MLatom 3 is a program package designed to leverage the power of ML to enhance typical computational chemistry simulations and to create complex workflows. This open-source package provides plenty of choice to the users who can run simulations with the command line options, input files, or with scripts using MLatom as a Python package, both on their computers and on the online XACS cloud computing at XACScloud.com. Computational chemists can calculate energies and thermochemical properties, optimize geometries, run molecular and quantum dynamics, and simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra with ML, quantum mechanical, and combined models. The users can choose from an extensive library of methods containing pre-trained ML models and quantum mechanical approximations such as AIQM1 approaching coupled-cluster accuracy. The developers can build their own models using various ML algorithms. The great flexibility of MLatom is largely due to the extensive use of the interfaces to many state-of-the-art software packages and libraries.</p></details> |  |
| **[COMET: A Domain-Specific Compilation of High-Performance Computational Chemistry](https://arxiv.org/pdf/2102.06827v1)** | 2022-01-02 | <details><summary>Show</summary><p>The computational power increases over the past decades havegreatly enhanced the ability to simulate chemical reactions andunderstand ever more complex transformations. Tensor contractions are the fundamental computational building block of these simulations. These simulations have often been tied to one platform and restricted in generality by the interface provided to the user. The expanding prevalence of accelerators and researcher demands necessitate a more general approach which is not tied to specific hardware or requires contortion of algorithms to specific hardware platforms. In this paper we present COMET, a domain-specific programming language and compiler infrastructure for tensor contractions targeting heterogeneous accelerators. We present a system of progressive lowering through multiple layers of abstraction and optimization that achieves up to 1.98X speedup for 30 tensor contractions commonly used in computational chemistry and beyond.</p></details> | <details><summary>Proce...</summary><p>Proceeding of the 33rd the Workshop on Languages and Compilers for Parallel Computing (LCPC), October 2020</p></details> |
| **[Towards the Practical Application of Near-Term Quantum Computers in Quantum Chemistry Simulations: A Problem Decomposition Approach](https://arxiv.org/pdf/1806.01305v1)** | 2018-06-06 | <details><summary>Show</summary><p>With the aim of establishing a framework to efficiently perform the practical application of quantum chemistry simulation on near-term quantum devices, we envision a hybrid quantum--classical framework for leveraging problem decomposition (PD) techniques in quantum chemistry. Specifically, we use PD techniques to decompose a target molecular system into smaller subsystems requiring fewer computational resources. In our framework, there are two levels of hybridization. At the first level, we use a classical algorithm to decompose a target molecule into subsystems, and utilize a quantum algorithm to simulate the quantum nature of the subsystems. The second level is in the quantum algorithm. We consider the quantum--classical variational algorithm that iterates between an expectation estimation using a quantum device and a parameter optimization using a classical device. We investigate three popular PD techniques for our hybrid approach: the fragment molecular-orbital (FMO) method, the divide-and-conquer (DC) technique, and the density matrix embedding theory (DMET). We examine the efficacy of these techniques in correctly differentiating conformations of simple alkane molecules. In particular, we consider the ratio between the number of qubits for PD and that of the full system; the mean absolute deviation; and the Pearson correlation coefficient and Spearman's rank correlation coefficient. Sampling error is introduced when expectation values are measured on the quantum device. Therefore, we study how this error affects the predictive performance of PD techniques. The present study is our first step to opening up the possibility of using quantum chemistry simulations at a scale close to the size of molecules relevant to industry on near-term quantum hardware.</p></details> |  |
| **[Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations](https://arxiv.org/pdf/2509.20667v1)** | 2025-09-26 | <details><summary>Show</summary><p>In this work, we develop machine learning (ML) based strategies to predict resources (costs) required for massively parallel chemistry computations, such as coupled-cluster methods, to guide application users before they commit to running expensive experiments on a supercomputer. By predicting application execution time, we determine the optimal runtime parameter values such as number of nodes and tile sizes. Two key questions of interest to users are addressed. The first is the shortest-time question, where the user is interested in knowing the parameter configurations (number of nodes and tile sizes) to achieve the shortest execution time for a given problem size and a target supercomputer. The second is the cheapest-run question in which the user is interested in minimizing resource usage, i.e., finding the number of nodes and tile size that minimizes the number of node-hours for a given problem size. We evaluate a rich family of ML models and strategies, developed based on the collections of runtime parameter values for the CCSD (Coupled Cluster with Singles and Doubles) application executed on the Department of Energy (DOE) Frontier and Aurora supercomputers. Our experiments show that when predicting the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora and Frontier, respectively. In the case where it is expensive to run experiments just to collect data points, we show that active learning can achieve a MAPE of about 0.2 with just around 450 experiments collected from Aurora and Frontier.</p></details> |  |
| **[Probabilistic performance estimators for computational chemistry methods: Systematic Improvement Probability and Ranking Probability Matrix. I. Theory](https://arxiv.org/pdf/2003.00987v4)** | 2020-09-29 | <details><summary>Show</summary><p>The comparison of benchmark error sets is an essential tool for the evaluation of theories in computational chemistry. The standard ranking of methods by their Mean Unsigned Error is unsatisfactory for several reasons linked to the non-normality of the error distributions and the presence of underlying trends. Complementary statistics have recently been proposed to palliate such deficiencies, such as quantiles of the absolute errors distribution or the mean prediction uncertainty. We introduce here a new score, the systematic improvement probability (SIP), based on the direct system-wise comparison of absolute errors. Independently of the chosen scoring rule, the uncertainty of the statistics due to the incompleteness of the benchmark data sets is also generally overlooked. However, this uncertainty is essential to appreciate the robustness of rankings. In the present article, we develop two indicators based on robust statistics to address this problem: P_{inv}, the inversion probability between two values of a statistic, and \mathbf{P}_{r}, the ranking probability matrix. We demonstrate also the essential contribution of the correlations between error sets in these scores comparisons.</p></details> |  |
| **[Computational Chemistry on Quantum Computers](https://arxiv.org/pdf/1909.11146v1)** | 2019-09-26 | <details><summary>Show</summary><p>The purpose of this experiment was to use the known analytical techniques to study the creation, simulation, and measurements of molecular Hamiltonians. The techniques used consisted of the Linear Combination of Atomic Orbitals (LCAO), the Linear Combination of Unitaries (LCU), and the Phase Estimation Algorithm (PEA). The molecules studied were $H_2$ with and without spin, as well as $He_2$ without spin. Hamiltonians were created under the LCAO basis, and reconstructed using the Jordan-Winger transform in order to create a linear combination of Pauli spin operators. The lengths of each molecular Hamiltonian greatly increased from the $H_2$ without spin, to $He_2$. This resulted in a reduced ability to simulate the Hamiltonians under ideal conditions. Thus, only low orders of l = 1 and l = 2 were used when expanding the Hamiltonian in accordance to the LCU method of simulation. The resulting Hamiltonians were measured using PEA, and plotted against function of $\frac{2π(K)}{N}$ and the probability distribution of each register. The resolution of the graph was dependent on the amount of registers, N, being used. However, the reduction of order hardly changed the image of the $H_2$ graphs. Qualitative comparisons between the three molecules were drawn.</p></details> | 10 pages, 4 figures |
| **[Computer vision-based recognition of liquid surfaces and phase boundaries in transparent vessels, with emphasis on chemistry applications](https://arxiv.org/pdf/1404.7174v7)** | 2014-11-10 | <details><summary>Show</summary><p>The ability to recognize the liquid surface and the liquid level in transparent containers is perhaps the most commonly used evaluation method when dealing with fluids. Such recognition is essential in determining the liquid volume, fill level, phase boundaries and phase separation in various fluid systems. The recognition of liquid surfaces is particularly important in solution chemistry, where it is essential to many laboratory techniques (e.g., extraction, distillation, titration). A general method for the recognition of interfaces between liquid and air or between phase-separating liquids could have a wide range of applications and contribute to the understanding of the visual properties of such interfaces. This work examines a computer vision method for the recognition of liquid surfaces and liquid levels in various transparent containers. The method can be applied to recognition of both liquid-air and liquid-liquid surfaces. No prior knowledge of the number of phases is required. The method receives the image of the liquid container and the boundaries of the container in the image and scans all possible curves that could correspond to the outlines of liquid surfaces in the image. The method then compares each curve to the image to rate its correspondence with the outline of the real liquid surface by examining various image properties in the area surrounding each point of the curve. The image properties that were found to give the best indication of the liquid surface are the relative intensity change, the edge density change and the gradient direction relative to the curve normal.</p></details> | <details><summary>Sourc...</summary><p>Source code for phase boundary and liquid surface recognition available at: http://www.mathworks.com/matlabcentral/fileexchange/46893-computer-vision-based-recognition-of-liquid-surface-and-liquid-level-of-liquid-of-transparent-vessel</p></details> |

